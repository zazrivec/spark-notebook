{
  "metadata" : {
    "name" : "Follow them all",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : [ "import scala.concurrent._", "import scala.concurrent.ExecutionContext.Implicits.global" ],
    "customArgs" : null,
    "customSparkConf" : {
      "spark.scheduler.mode" : "FAIR"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Check several jobs under FAIR scheduler condition"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Using Fair scheduler to schedule more than one job's tasks at a given time."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Setup"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We create a dummy dataset"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val rdd = sparkContext.parallelize(1 to 10000)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:51\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:51"
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Parallel jobs (using `Future`)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "Future{ rdd.mapPartitions{ x => Thread.sleep(scala.util.Random.nextInt(10000)); x}.count }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res2: scala.concurrent.Future[Long] = scala.concurrent.impl.Promise$DefaultPromise@19597744\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "scala.concurrent.impl.Promise$DefaultPromise@19597744"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "Future{ \n  rdd.mapPartitions{ x => Thread.sleep(scala.util.Random.nextInt(10000)); x}\n   .count\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res3: scala.concurrent.Future[Long] = scala.concurrent.impl.Promise$DefaultPromise@294e621b\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "scala.concurrent.impl.Promise$DefaultPromise@294e621b"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "Future{ \n  rdd.repartition(12).mapPartitions{ x => Thread.sleep(scala.util.Random.nextInt(10000)); x}\n   .count\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res5: scala.concurrent.Future[Long] = scala.concurrent.impl.Promise$DefaultPromise@12268f99\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "scala.concurrent.impl.Promise$DefaultPromise@12268f99"
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  } ],
  "nbformat" : 4
}